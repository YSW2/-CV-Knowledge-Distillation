{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNW9tjyJp+1qb++CiwSJ/uo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YSW2/CV-KnowledgeDistillation/blob/master/TAKD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "# Check if GPU is available, and if not, use the CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "sxUbUGdNbRXP"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, epochs, learning_rate, device):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True, weight_decay=1e-4)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            # inputs: A collection of batch_size images\n",
        "            # labels: A vector of dimensionality batch_size with integers denoting class of each image\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # outputs: Output of the network for the collection of images. A tensor of dimensionality batch_size x num_classes\n",
        "            # labels: The actual labels of the images. Vector of dimensionality batch_size\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n",
        "\n",
        "def test(model, test_loader, device):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "bX2wiSCTgCAH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8mPg6Qu0af5p"
      },
      "outputs": [],
      "source": [
        "def data_loader(num_classes=10):\n",
        "  # Below we are preprocessing data for CIFAR-10. We use an arbitrary batch size of 128.\n",
        "  transforms_cifar = transforms.Compose([\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "  ])\n",
        "\n",
        "  if num_classes == 10:\n",
        "    # Loading the CIFAR-10 dataset:\n",
        "    train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms_cifar)\n",
        "    test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms_cifar)\n",
        "\n",
        "  elif num_classes == 100:\n",
        "    train_dataset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transforms_cifar)\n",
        "    test_dataset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transforms_cifar)\n",
        "\n",
        "  return train_dataset, test_dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvNetMaker(nn.Module):\n",
        "    \"\"\"\n",
        "    Creates a convolutional neural network (CNN) based on a given specification of layers.\n",
        "    \"\"\"\n",
        "    def __init__(self, layers):\n",
        "        \"\"\"\n",
        "        Initializes the CNN model with the specified layers.\n",
        "\n",
        "        :param layers: A list of strings, each representing a layer specification,\n",
        "                       such as [\"Conv64\", \"MaxPool\", \"Conv128\", \"MaxPool\", \"FC100\", \"FC10\"].\n",
        "                       \"Conv64\" means a convolutional layer with 64 filters,\n",
        "                       \"MaxPool\" means a max pooling layer,\n",
        "                       \"FC100\" means a fully connected layer with 100 neurons.\n",
        "        \"\"\"\n",
        "        super(ConvNetMaker, self).__init__()\n",
        "        self.conv_layers = []\n",
        "        self.fc_layers = []\n",
        "        h, w, d = 32, 32, 3\n",
        "        previous_layer_filter_count = 3\n",
        "        previous_layer_size = h * w * d\n",
        "        num_fc_layers_remained = len([1 for l in layers if l.startswith(\"FC\")])\n",
        "        for layer in layers:\n",
        "            if layer.startswith(\"Conv\"):\n",
        "                filter_count = int(layer[4:])\n",
        "                self.conv_layers += [\n",
        "                    nn.Conv2d(\n",
        "                        previous_layer_filter_count,\n",
        "                        filter_count,\n",
        "                        kernel_size=3,\n",
        "                        padding=1,\n",
        "                    ),\n",
        "                    nn.BatchNorm2d(filter_count),\n",
        "                    nn.ReLU(inplace=True),\n",
        "                ]\n",
        "                previous_layer_filter_count = filter_count\n",
        "                d = filter_count\n",
        "                previous_layer_size = h * w * d\n",
        "            elif layer.startswith(\"MaxPool\"):\n",
        "                self.conv_layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "                h, w = int(h / 2.0), int(w / 2.0)\n",
        "                previous_layer_size = h * w * d\n",
        "            elif layer.startswith(\"FC\"):\n",
        "                num_fc_layers_remained -= 1\n",
        "                current_layer_size = int(layer[2:])\n",
        "                if num_fc_layers_remained == 0:\n",
        "                    self.fc_layers += [\n",
        "                        nn.Linear(previous_layer_size, current_layer_size)\n",
        "                    ]\n",
        "                else:\n",
        "                    self.fc_layers += [\n",
        "                        nn.Linear(previous_layer_size, current_layer_size),\n",
        "                        nn.ReLU(inplace=True),\n",
        "                    ]\n",
        "                previous_layer_size = current_layer_size\n",
        "\n",
        "        conv_layers = self.conv_layers\n",
        "        fc_layers = self.fc_layers\n",
        "        self.conv_layers = nn.Sequential(*conv_layers)\n",
        "        self.fc_layers = nn.Sequential(*fc_layers)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Defines the forward pass of the model.\n",
        "\n",
        "        :param x: Input tensor of shape (batch_size, 3, 32, 32)\n",
        "        :return: Output tensor\n",
        "        \"\"\"\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc_layers(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "5Vta5updbKCH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plane_cifar10_book = {\n",
        "\t'2': ['Conv16', 'MaxPool', 'Conv16', 'MaxPool', 'FC10'],\n",
        "\t'4': ['Conv16', 'Conv16', 'MaxPool', 'Conv32', 'Conv32', 'MaxPool', 'FC10'],\n",
        "\t'6': ['Conv16', 'Conv16', 'MaxPool', 'Conv32', 'Conv32', 'MaxPool', 'Conv64', 'Conv64', 'MaxPool', 'FC10'],\n",
        "\t'8': ['Conv16', 'Conv16', 'MaxPool', 'Conv32', 'Conv32', 'MaxPool', 'Conv64', 'Conv64', 'MaxPool',\n",
        "\t\t  'Conv128', 'Conv128','MaxPool', 'FC64', 'FC10'],\n",
        "\t'10': ['Conv32', 'Conv32', 'MaxPool', 'Conv64', 'Conv64', 'MaxPool', 'Conv128', 'Conv128', 'MaxPool',\n",
        "\t\t   'Conv256', 'Conv256', 'Conv256', 'Conv256' , 'MaxPool', 'FC128' ,'FC10'],\n",
        "}\n",
        "\n",
        "\n",
        "plane_cifar100_book = {\n",
        "\t'2': ['Conv32', 'MaxPool', 'Conv32', 'MaxPool', 'FC100'],\n",
        "\t'4': ['Conv32', 'Conv32', 'MaxPool', 'Conv64', 'Conv64', 'MaxPool', 'FC100'],\n",
        "\t'6': ['Conv32', 'Conv32', 'MaxPool', 'Conv64', 'Conv64', 'MaxPool','Conv128', 'Conv128' ,'FC100'],\n",
        "\t'8': ['Conv32', 'Conv32', 'MaxPool', 'Conv64', 'Conv64', 'MaxPool', 'Conv128', 'Conv128', 'MaxPool',\n",
        "\t\t  'Conv256', 'Conv256','MaxPool', 'FC64', 'FC100'],\n",
        "\t'10': ['Conv32', 'Conv32', 'MaxPool', 'Conv64', 'Conv64', 'MaxPool', 'Conv128', 'Conv128', 'MaxPool',\n",
        "\t\t   'Conv256', 'Conv256', 'Conv256', 'Conv256' , 'MaxPool', 'FC512', 'FC100'],\n",
        "}"
      ],
      "metadata": {
        "id": "NloxAxsqhl8c"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dataloaders\n",
        "train_dataset, test_dataset = data_loader(100)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yg1ENrEngwbm",
        "outputId": "f593e48b-210f-4014-a495-27ad5b9a0b51"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169001437/169001437 [00:03<00:00, 42756795.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-100-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "layer = plane_cifar100_book.get('10')\n",
        "\n",
        "teacher_model = ConvNetMaker(layer).to(device)\n",
        "train(teacher_model, train_loader, epochs=160, learning_rate=0.1, device=device)\n",
        "test_accuracy_deep = test(teacher_model, test_loader, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BagGMs5EgJ7i",
        "outputId": "85cde64a-71ac-40f8-bb05-3fb545f45d6f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/160, Loss: 4.0023466302915605\n",
            "Epoch 2/160, Loss: 3.3825098907246307\n",
            "Epoch 3/160, Loss: 2.837200911148735\n",
            "Epoch 4/160, Loss: 2.4425559885361614\n",
            "Epoch 5/160, Loss: 2.1535565828728247\n",
            "Epoch 6/160, Loss: 1.9373015598262973\n",
            "Epoch 7/160, Loss: 1.752702307213298\n",
            "Epoch 8/160, Loss: 1.5946563842046597\n",
            "Epoch 9/160, Loss: 1.453705199844087\n",
            "Epoch 10/160, Loss: 1.3163965981634682\n",
            "Epoch 11/160, Loss: 1.2052330216178504\n",
            "Epoch 12/160, Loss: 1.104205213544314\n",
            "Epoch 13/160, Loss: 0.9951046691526233\n",
            "Epoch 14/160, Loss: 0.9089377065143927\n",
            "Epoch 15/160, Loss: 0.831626842546341\n",
            "Epoch 16/160, Loss: 0.7648328394841051\n",
            "Epoch 17/160, Loss: 0.6993429978637744\n",
            "Epoch 18/160, Loss: 0.6465785038440733\n",
            "Epoch 19/160, Loss: 0.580706291262756\n",
            "Epoch 20/160, Loss: 0.554457656105461\n",
            "Epoch 21/160, Loss: 0.5211712013730003\n",
            "Epoch 22/160, Loss: 0.4704688175407517\n",
            "Epoch 23/160, Loss: 0.45097348997202674\n",
            "Epoch 24/160, Loss: 0.4142204585587582\n",
            "Epoch 25/160, Loss: 0.3983548833890949\n",
            "Epoch 26/160, Loss: 0.3866790081457714\n",
            "Epoch 27/160, Loss: 0.3596067783396567\n",
            "Epoch 28/160, Loss: 0.34039645697302223\n",
            "Epoch 29/160, Loss: 0.32094055539964106\n",
            "Epoch 30/160, Loss: 0.31214479597099604\n",
            "Epoch 31/160, Loss: 0.2910916963044335\n",
            "Epoch 32/160, Loss: 0.2904110669213183\n",
            "Epoch 33/160, Loss: 0.2871614743376632\n",
            "Epoch 34/160, Loss: 0.2817156871547327\n",
            "Epoch 35/160, Loss: 0.2785391187500161\n",
            "Epoch 36/160, Loss: 0.24654752690621348\n",
            "Epoch 37/160, Loss: 0.2643157923617936\n",
            "Epoch 38/160, Loss: 0.2493765225724491\n",
            "Epoch 39/160, Loss: 0.2452755800217314\n",
            "Epoch 40/160, Loss: 0.253457397031967\n",
            "Epoch 41/160, Loss: 0.23727337862638867\n",
            "Epoch 42/160, Loss: 0.24291057600770766\n",
            "Epoch 43/160, Loss: 0.2392142778048125\n",
            "Epoch 44/160, Loss: 0.24656643680370677\n",
            "Epoch 45/160, Loss: 0.21861250306029453\n",
            "Epoch 46/160, Loss: 0.22441442201242728\n",
            "Epoch 47/160, Loss: 0.22681876410113272\n",
            "Epoch 48/160, Loss: 0.23165332750819834\n",
            "Epoch 49/160, Loss: 0.22291874367257822\n",
            "Epoch 50/160, Loss: 0.2264252782458692\n",
            "Epoch 51/160, Loss: 0.21452926809106337\n",
            "Epoch 52/160, Loss: 0.2279443141558896\n",
            "Epoch 53/160, Loss: 0.2077403902111913\n",
            "Epoch 54/160, Loss: 0.22757015415393483\n",
            "Epoch 55/160, Loss: 0.21835029296710362\n",
            "Epoch 56/160, Loss: 0.21957775401642254\n",
            "Epoch 57/160, Loss: 0.20601198779385718\n",
            "Epoch 58/160, Loss: 0.21507568217223258\n",
            "Epoch 59/160, Loss: 0.20381262016189677\n",
            "Epoch 60/160, Loss: 0.21154941757545448\n",
            "Epoch 61/160, Loss: 0.21355337942081035\n",
            "Epoch 62/160, Loss: 0.21457101658100972\n",
            "Epoch 63/160, Loss: 0.19616408427924756\n",
            "Epoch 64/160, Loss: 0.21393675585765667\n",
            "Epoch 65/160, Loss: 0.20679288644276922\n",
            "Epoch 66/160, Loss: 0.2071593238131317\n",
            "Epoch 67/160, Loss: 0.20340800982759433\n",
            "Epoch 68/160, Loss: 0.2005455531275181\n",
            "Epoch 69/160, Loss: 0.1997261250872746\n",
            "Epoch 70/160, Loss: 0.1931810829111987\n",
            "Epoch 71/160, Loss: 0.20529706190194924\n",
            "Epoch 72/160, Loss: 0.20866985382783748\n",
            "Epoch 73/160, Loss: 0.1955974015319134\n",
            "Epoch 74/160, Loss: 0.19169497594732762\n",
            "Epoch 75/160, Loss: 0.19857808048158046\n",
            "Epoch 76/160, Loss: 0.1801130734574612\n",
            "Epoch 77/160, Loss: 0.19632530637333157\n",
            "Epoch 78/160, Loss: 0.19876084975002672\n",
            "Epoch 79/160, Loss: 0.20472731172581157\n",
            "Epoch 80/160, Loss: 0.1888984476342378\n",
            "Epoch 81/160, Loss: 0.184229764287048\n",
            "Epoch 82/160, Loss: 0.18787376603583242\n",
            "Epoch 83/160, Loss: 0.19849714254269668\n",
            "Epoch 84/160, Loss: 0.1787134350546638\n",
            "Epoch 85/160, Loss: 0.18997070713497488\n",
            "Epoch 86/160, Loss: 0.20131896864956297\n",
            "Epoch 87/160, Loss: 0.20410458094742903\n",
            "Epoch 88/160, Loss: 0.1748730716726664\n",
            "Epoch 89/160, Loss: 0.19415990276562284\n",
            "Epoch 90/160, Loss: 0.19355598635152174\n",
            "Epoch 91/160, Loss: 0.18836848462557854\n",
            "Epoch 92/160, Loss: 0.19209978506540704\n",
            "Epoch 93/160, Loss: 0.18585637405209834\n",
            "Epoch 94/160, Loss: 0.18322433443630443\n",
            "Epoch 95/160, Loss: 0.18996046240562978\n",
            "Epoch 96/160, Loss: 0.18512500593881778\n",
            "Epoch 97/160, Loss: 0.1734209926346379\n",
            "Epoch 98/160, Loss: 0.19738594349235525\n",
            "Epoch 99/160, Loss: 0.18761410003961504\n",
            "Epoch 100/160, Loss: 0.1818453482902416\n",
            "Epoch 101/160, Loss: 0.18914331400485904\n",
            "Epoch 102/160, Loss: 0.18088878360588836\n",
            "Epoch 103/160, Loss: 0.17976483534973905\n",
            "Epoch 104/160, Loss: 0.17356728003991534\n",
            "Epoch 105/160, Loss: 0.18281371743820818\n",
            "Epoch 106/160, Loss: 0.18733508690543796\n",
            "Epoch 107/160, Loss: 0.1719754150094431\n",
            "Epoch 108/160, Loss: 0.1783083325914105\n",
            "Epoch 109/160, Loss: 0.18642044497077423\n",
            "Epoch 110/160, Loss: 0.18763185387758344\n",
            "Epoch 111/160, Loss: 0.17101718803577107\n",
            "Epoch 112/160, Loss: 0.17262900434434414\n",
            "Epoch 113/160, Loss: 0.1751075789160893\n",
            "Epoch 114/160, Loss: 0.17696899042257566\n",
            "Epoch 115/160, Loss: 0.16947501847315627\n",
            "Epoch 116/160, Loss: 0.17520968724623362\n",
            "Epoch 117/160, Loss: 0.18350494605348544\n",
            "Epoch 118/160, Loss: 0.17874977683357876\n",
            "Epoch 119/160, Loss: 0.18620426300198525\n",
            "Epoch 120/160, Loss: 0.15920235058935858\n",
            "Epoch 121/160, Loss: 0.18962771556985653\n",
            "Epoch 122/160, Loss: 0.17985160812697448\n",
            "Epoch 123/160, Loss: 0.17710623564317707\n",
            "Epoch 124/160, Loss: 0.17045906456687565\n",
            "Epoch 125/160, Loss: 0.17791567592288526\n",
            "Epoch 126/160, Loss: 0.1686667719727282\n",
            "Epoch 127/160, Loss: 0.1788641058785074\n",
            "Epoch 128/160, Loss: 0.1640551581531001\n",
            "Epoch 129/160, Loss: 0.1703322949673971\n",
            "Epoch 130/160, Loss: 0.1770181867586987\n",
            "Epoch 131/160, Loss: 0.17042185055554065\n",
            "Epoch 132/160, Loss: 0.1677806951924968\n",
            "Epoch 133/160, Loss: 0.18414863785895544\n",
            "Epoch 134/160, Loss: 0.17884794048145605\n",
            "Epoch 135/160, Loss: 0.16196413109522037\n",
            "Epoch 136/160, Loss: 0.16871637609951637\n",
            "Epoch 137/160, Loss: 0.16225023358069418\n",
            "Epoch 138/160, Loss: 0.15978682548989115\n",
            "Epoch 139/160, Loss: 0.17335152775620866\n",
            "Epoch 140/160, Loss: 0.18000442485141632\n",
            "Epoch 141/160, Loss: 0.16736923687903169\n",
            "Epoch 142/160, Loss: 0.15713490889695905\n",
            "Epoch 143/160, Loss: 0.17452604554193404\n",
            "Epoch 144/160, Loss: 0.17757981119062893\n",
            "Epoch 145/160, Loss: 0.17274090888745644\n",
            "Epoch 146/160, Loss: 0.1762942568925412\n",
            "Epoch 147/160, Loss: 0.17281628602072405\n",
            "Epoch 148/160, Loss: 0.17004052676317638\n",
            "Epoch 149/160, Loss: 0.15157592125103603\n",
            "Epoch 150/160, Loss: 0.16275920702711397\n",
            "Epoch 151/160, Loss: 0.16867457112997694\n",
            "Epoch 152/160, Loss: 0.1685743616974872\n",
            "Epoch 153/160, Loss: 0.15644780403989203\n",
            "Epoch 154/160, Loss: 0.1571132522600386\n",
            "Epoch 155/160, Loss: 0.1620321794963249\n",
            "Epoch 156/160, Loss: 0.14856321922958354\n",
            "Epoch 157/160, Loss: 0.1486237078543057\n",
            "Epoch 158/160, Loss: 0.16927593864995957\n",
            "Epoch 159/160, Loss: 0.17663527331541262\n",
            "Epoch 160/160, Loss: 0.16409819947598536\n",
            "Test Accuracy: 52.15%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "layer = plane_cifar100_book.get('2')\n",
        "torch.manual_seed(42)\n",
        "student_model = ConvNetMaker(layer).to(device)\n",
        "\n",
        "train(student_model, train_loader, epochs=160, learning_rate=0.1, device=device)\n",
        "test_accuracy_light_ce = test(student_model, test_loader, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5haIDBJ2kIp5",
        "outputId": "62c3e993-a9e2-44ff-a545-245c1ef2661c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/160, Loss: 4.239775285696434\n",
            "Epoch 2/160, Loss: 3.8019423356751347\n",
            "Epoch 3/160, Loss: 3.651276835395247\n",
            "Epoch 4/160, Loss: 3.5618112514086087\n",
            "Epoch 5/160, Loss: 3.489645529281148\n",
            "Epoch 6/160, Loss: 3.4355800554270632\n",
            "Epoch 7/160, Loss: 3.389972392860276\n",
            "Epoch 8/160, Loss: 3.3560808839090646\n",
            "Epoch 9/160, Loss: 3.3215228322216923\n",
            "Epoch 10/160, Loss: 3.2897250896219705\n",
            "Epoch 11/160, Loss: 3.270614765489193\n",
            "Epoch 12/160, Loss: 3.250949508393817\n",
            "Epoch 13/160, Loss: 3.2386700344817414\n",
            "Epoch 14/160, Loss: 3.220334542072033\n",
            "Epoch 15/160, Loss: 3.2080346989204815\n",
            "Epoch 16/160, Loss: 3.1948150753060265\n",
            "Epoch 17/160, Loss: 3.1832464399849973\n",
            "Epoch 18/160, Loss: 3.173683879930345\n",
            "Epoch 19/160, Loss: 3.165260699094104\n",
            "Epoch 20/160, Loss: 3.1542178944248676\n",
            "Epoch 21/160, Loss: 3.1503302953432284\n",
            "Epoch 22/160, Loss: 3.14686003060597\n",
            "Epoch 23/160, Loss: 3.137268679221268\n",
            "Epoch 24/160, Loss: 3.1337230327489127\n",
            "Epoch 25/160, Loss: 3.12435141731711\n",
            "Epoch 26/160, Loss: 3.1241686295365434\n",
            "Epoch 27/160, Loss: 3.1239989358750755\n",
            "Epoch 28/160, Loss: 3.1163922813542357\n",
            "Epoch 29/160, Loss: 3.114788495671109\n",
            "Epoch 30/160, Loss: 3.1098517871573756\n",
            "Epoch 31/160, Loss: 3.106331974039297\n",
            "Epoch 32/160, Loss: 3.103819150144182\n",
            "Epoch 33/160, Loss: 3.104112424508995\n",
            "Epoch 34/160, Loss: 3.0998243492887454\n",
            "Epoch 35/160, Loss: 3.0955489145215513\n",
            "Epoch 36/160, Loss: 3.091472975128447\n",
            "Epoch 37/160, Loss: 3.0906027034115606\n",
            "Epoch 38/160, Loss: 3.0914458033373897\n",
            "Epoch 39/160, Loss: 3.0866317035597\n",
            "Epoch 40/160, Loss: 3.0840936158319265\n",
            "Epoch 41/160, Loss: 3.087006851840202\n",
            "Epoch 42/160, Loss: 3.083589137362702\n",
            "Epoch 43/160, Loss: 3.0846515598199558\n",
            "Epoch 44/160, Loss: 3.0792110326040127\n",
            "Epoch 45/160, Loss: 3.077279615890035\n",
            "Epoch 46/160, Loss: 3.0763257312042938\n",
            "Epoch 47/160, Loss: 3.0744530587550014\n",
            "Epoch 48/160, Loss: 3.0712149405418456\n",
            "Epoch 49/160, Loss: 3.071326897882135\n",
            "Epoch 50/160, Loss: 3.0723226527728693\n",
            "Epoch 51/160, Loss: 3.0681685120858195\n",
            "Epoch 52/160, Loss: 3.0685851512967472\n",
            "Epoch 53/160, Loss: 3.074652968160332\n",
            "Epoch 54/160, Loss: 3.062877472709207\n",
            "Epoch 55/160, Loss: 3.063047591377707\n",
            "Epoch 56/160, Loss: 3.062704928390815\n",
            "Epoch 57/160, Loss: 3.060163883297035\n",
            "Epoch 58/160, Loss: 3.062171264072818\n",
            "Epoch 59/160, Loss: 3.063186882706859\n",
            "Epoch 60/160, Loss: 3.0576929082650968\n",
            "Epoch 61/160, Loss: 3.056344255462022\n",
            "Epoch 62/160, Loss: 3.061370721558476\n",
            "Epoch 63/160, Loss: 3.0566513568848905\n",
            "Epoch 64/160, Loss: 3.0566092372855262\n",
            "Epoch 65/160, Loss: 3.050663470612158\n",
            "Epoch 66/160, Loss: 3.056285575222786\n",
            "Epoch 67/160, Loss: 3.053895774704721\n",
            "Epoch 68/160, Loss: 3.051457647167508\n",
            "Epoch 69/160, Loss: 3.054078006378525\n",
            "Epoch 70/160, Loss: 3.048990454515228\n",
            "Epoch 71/160, Loss: 3.049623474135728\n",
            "Epoch 72/160, Loss: 3.0490358699008326\n",
            "Epoch 73/160, Loss: 3.050452471389185\n",
            "Epoch 74/160, Loss: 3.0464443020198657\n",
            "Epoch 75/160, Loss: 3.0439601609164186\n",
            "Epoch 76/160, Loss: 3.043490579366074\n",
            "Epoch 77/160, Loss: 3.0438845627143256\n",
            "Epoch 78/160, Loss: 3.0475317919650653\n",
            "Epoch 79/160, Loss: 3.041749081038453\n",
            "Epoch 80/160, Loss: 3.0472630155665796\n",
            "Epoch 81/160, Loss: 3.0413329095181907\n",
            "Epoch 82/160, Loss: 3.0386006393091147\n",
            "Epoch 83/160, Loss: 3.0416005785812805\n",
            "Epoch 84/160, Loss: 3.0362121107633158\n",
            "Epoch 85/160, Loss: 3.040771252663849\n",
            "Epoch 86/160, Loss: 3.0368610647938135\n",
            "Epoch 87/160, Loss: 3.041404649729619\n",
            "Epoch 88/160, Loss: 3.0386899334695334\n",
            "Epoch 89/160, Loss: 3.04096582234668\n",
            "Epoch 90/160, Loss: 3.035467302097994\n",
            "Epoch 91/160, Loss: 3.0379774119238108\n",
            "Epoch 92/160, Loss: 3.03514947915626\n",
            "Epoch 93/160, Loss: 3.0324856314207893\n",
            "Epoch 94/160, Loss: 3.034196634731634\n",
            "Epoch 95/160, Loss: 3.0373990133290403\n",
            "Epoch 96/160, Loss: 3.0328244986131674\n",
            "Epoch 97/160, Loss: 3.0315209532637732\n",
            "Epoch 98/160, Loss: 3.0341459058434763\n",
            "Epoch 99/160, Loss: 3.032800731146732\n",
            "Epoch 100/160, Loss: 3.031607558965073\n",
            "Epoch 101/160, Loss: 3.032792320031949\n",
            "Epoch 102/160, Loss: 3.0318702572142073\n",
            "Epoch 103/160, Loss: 3.0305455956617586\n",
            "Epoch 104/160, Loss: 3.0203359633150613\n",
            "Epoch 105/160, Loss: 2.957491150292594\n",
            "Epoch 106/160, Loss: 2.902548268018171\n",
            "Epoch 107/160, Loss: 2.8695754139014826\n",
            "Epoch 108/160, Loss: 2.8550719495319647\n",
            "Epoch 109/160, Loss: 2.842036639333076\n",
            "Epoch 110/160, Loss: 2.834571065195381\n",
            "Epoch 111/160, Loss: 2.822880780910287\n",
            "Epoch 112/160, Loss: 2.815020471582632\n",
            "Epoch 113/160, Loss: 2.814486780434923\n",
            "Epoch 114/160, Loss: 2.808525976317618\n",
            "Epoch 115/160, Loss: 2.806015671976387\n",
            "Epoch 116/160, Loss: 2.8038727706655515\n",
            "Epoch 117/160, Loss: 2.7991716928799133\n",
            "Epoch 118/160, Loss: 2.792942919084788\n",
            "Epoch 119/160, Loss: 2.793114475581957\n",
            "Epoch 120/160, Loss: 2.787324090747882\n",
            "Epoch 121/160, Loss: 2.790440131331344\n",
            "Epoch 122/160, Loss: 2.783805688018994\n",
            "Epoch 123/160, Loss: 2.7863003555161265\n",
            "Epoch 124/160, Loss: 2.7845418812978604\n",
            "Epoch 125/160, Loss: 2.78170066904229\n",
            "Epoch 126/160, Loss: 2.7798094822622628\n",
            "Epoch 127/160, Loss: 2.778625818164757\n",
            "Epoch 128/160, Loss: 2.777354128830268\n",
            "Epoch 129/160, Loss: 2.7734044890879366\n",
            "Epoch 130/160, Loss: 2.774318361526255\n",
            "Epoch 131/160, Loss: 2.7713727920561495\n",
            "Epoch 132/160, Loss: 2.7710834269023614\n",
            "Epoch 133/160, Loss: 2.768069895941888\n",
            "Epoch 134/160, Loss: 2.762904840357163\n",
            "Epoch 135/160, Loss: 2.7675017243456046\n",
            "Epoch 136/160, Loss: 2.763729374731898\n",
            "Epoch 137/160, Loss: 2.7678893228321124\n",
            "Epoch 138/160, Loss: 2.770208482547184\n",
            "Epoch 139/160, Loss: 2.7636861459678395\n",
            "Epoch 140/160, Loss: 2.760574476188406\n",
            "Epoch 141/160, Loss: 2.7624542164375714\n",
            "Epoch 142/160, Loss: 2.764258160920399\n",
            "Epoch 143/160, Loss: 2.7535860087255686\n",
            "Epoch 144/160, Loss: 2.7577794854293396\n",
            "Epoch 145/160, Loss: 2.752584347029781\n",
            "Epoch 146/160, Loss: 2.7607984426991106\n",
            "Epoch 147/160, Loss: 2.754258785711225\n",
            "Epoch 148/160, Loss: 2.7540970671817164\n",
            "Epoch 149/160, Loss: 2.7501567721062\n",
            "Epoch 150/160, Loss: 2.748915130829872\n",
            "Epoch 151/160, Loss: 2.7500479282320613\n",
            "Epoch 152/160, Loss: 2.749798836305623\n",
            "Epoch 153/160, Loss: 2.75003498960334\n",
            "Epoch 154/160, Loss: 2.746406742983767\n",
            "Epoch 155/160, Loss: 2.748268573180489\n",
            "Epoch 156/160, Loss: 2.75077197070012\n",
            "Epoch 157/160, Loss: 2.742710656827063\n",
            "Epoch 158/160, Loss: 2.7484924805438733\n",
            "Epoch 159/160, Loss: 2.7402573735512736\n",
            "Epoch 160/160, Loss: 2.7472267955770273\n",
            "Test Accuracy: 26.77%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_knowledge_distillation(teacher, student, train_loader, epochs, learning_rate, T, soft_target_loss_weight, ce_loss_weight, device):\n",
        "    ce_loss = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(student.parameters(), lr=learning_rate, momentum=0.9, nesterov=True, weight_decay=1e-4)\n",
        "\n",
        "    teacher.eval()  # Teacher set to evaluation mode\n",
        "    student.train() # Student to train mode\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass with the teacher model - do not save gradients here as we do not change the teacher's weights\n",
        "            with torch.no_grad():\n",
        "                teacher_logits = teacher(inputs)\n",
        "\n",
        "            # Forward pass with the student model\n",
        "            student_logits = student(inputs)\n",
        "\n",
        "            #Soften the student logits by applying softmax first and log() second\n",
        "            soft_targets = nn.functional.softmax(teacher_logits / T, dim=-1)\n",
        "            soft_prob = nn.functional.log_softmax(student_logits / T, dim=-1)\n",
        "\n",
        "            # Calculate the soft targets loss. Scaled by T**2 as suggested by the authors of the paper \"Distilling the knowledge in a neural network\"\n",
        "            soft_targets_loss = -torch.sum(soft_targets * soft_prob) / soft_prob.size()[0] * (T**2)\n",
        "\n",
        "            # Calculate the true label loss\n",
        "            label_loss = ce_loss(student_logits, labels)\n",
        "\n",
        "            # Weighted sum of the two losses\n",
        "            loss = soft_target_loss_weight * soft_targets_loss + ce_loss_weight * label_loss\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n"
      ],
      "metadata": {
        "id": "GWszARHJBIZL"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply ``train_knowledge_distillation`` with a temperature of 2. Arbitrarily set the weights to 0.75 for CE and 0.25 for distillation loss.\n",
        "kd_student_model = ConvNetMaker(layer).to(device)\n",
        "train_knowledge_distillation(teacher=teacher_model, student=kd_student_model, train_loader=train_loader, epochs=160, learning_rate=0.1, T=2, soft_target_loss_weight=0.25, ce_loss_weight=0.75, device=device)\n",
        "test_accuracy_light_ce_and_kd = test(kd_student_model, test_loader, device)\n",
        "\n",
        "# Compare the student test accuracy with and without the teacher, after distillation\n",
        "print(f\"Teacher accuracy: {test_accuracy_deep:.2f}%\")\n",
        "print(f\"Student accuracy without teacher: {test_accuracy_light_ce:.2f}%\")\n",
        "print(f\"Student accuracy with CE + KD: {test_accuracy_light_ce_and_kd:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afIx7W2CBOCe",
        "outputId": "0e8b7827-56bf-42a0-a324-5ff3cdb00acb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/160, Loss: 7.300865008702973\n",
            "Epoch 2/160, Loss: 6.638971815328769\n",
            "Epoch 3/160, Loss: 6.403528075693818\n",
            "Epoch 4/160, Loss: 6.252380539389217\n",
            "Epoch 5/160, Loss: 6.163560833162664\n",
            "Epoch 6/160, Loss: 6.0971230923977044\n",
            "Epoch 7/160, Loss: 6.063027346530534\n",
            "Epoch 8/160, Loss: 6.0121019051203035\n",
            "Epoch 9/160, Loss: 5.930418932834245\n",
            "Epoch 10/160, Loss: 5.754262536383041\n",
            "Epoch 11/160, Loss: 5.649084627780768\n",
            "Epoch 12/160, Loss: 5.589612998620933\n",
            "Epoch 13/160, Loss: 5.548440801518042\n",
            "Epoch 14/160, Loss: 5.51570523791301\n",
            "Epoch 15/160, Loss: 5.484533177007495\n",
            "Epoch 16/160, Loss: 5.473247542710561\n",
            "Epoch 17/160, Loss: 5.453340045021623\n",
            "Epoch 18/160, Loss: 5.435664272064443\n",
            "Epoch 19/160, Loss: 5.42592137792836\n",
            "Epoch 20/160, Loss: 5.414791857190145\n",
            "Epoch 21/160, Loss: 5.400842664186912\n",
            "Epoch 22/160, Loss: 5.3958924664255905\n",
            "Epoch 23/160, Loss: 5.3903499930106165\n",
            "Epoch 24/160, Loss: 5.374655116244655\n",
            "Epoch 25/160, Loss: 5.376932133189248\n",
            "Epoch 26/160, Loss: 5.366078758483653\n",
            "Epoch 27/160, Loss: 5.366943319130431\n",
            "Epoch 28/160, Loss: 5.349156678485139\n",
            "Epoch 29/160, Loss: 5.346994350023587\n",
            "Epoch 30/160, Loss: 5.348904398701075\n",
            "Epoch 31/160, Loss: 5.341142784908909\n",
            "Epoch 32/160, Loss: 5.339903000980387\n",
            "Epoch 33/160, Loss: 5.337399060768849\n",
            "Epoch 34/160, Loss: 5.3371380344986\n",
            "Epoch 35/160, Loss: 5.326633933864896\n",
            "Epoch 36/160, Loss: 5.3231803255008\n",
            "Epoch 37/160, Loss: 5.328901825048734\n",
            "Epoch 38/160, Loss: 5.316043833027715\n",
            "Epoch 39/160, Loss: 5.316182191414601\n",
            "Epoch 40/160, Loss: 5.3120979470060306\n",
            "Epoch 41/160, Loss: 5.312575811010492\n",
            "Epoch 42/160, Loss: 5.30600295347326\n",
            "Epoch 43/160, Loss: 5.311285890886546\n",
            "Epoch 44/160, Loss: 5.303535212641177\n",
            "Epoch 45/160, Loss: 5.304250348864309\n",
            "Epoch 46/160, Loss: 5.301071259676648\n",
            "Epoch 47/160, Loss: 5.292406740700803\n",
            "Epoch 48/160, Loss: 5.300586438239993\n",
            "Epoch 49/160, Loss: 5.292142098517064\n",
            "Epoch 50/160, Loss: 5.3033924968651185\n",
            "Epoch 51/160, Loss: 5.2933882984054055\n",
            "Epoch 52/160, Loss: 5.284181693630755\n",
            "Epoch 53/160, Loss: 5.285444705992403\n",
            "Epoch 54/160, Loss: 5.287877693810426\n",
            "Epoch 55/160, Loss: 5.280944344942527\n",
            "Epoch 56/160, Loss: 5.2756119506133485\n",
            "Epoch 57/160, Loss: 5.281743617923668\n",
            "Epoch 58/160, Loss: 5.279913786427139\n",
            "Epoch 59/160, Loss: 5.275849024048242\n",
            "Epoch 60/160, Loss: 5.274100334138212\n",
            "Epoch 61/160, Loss: 5.276645467714276\n",
            "Epoch 62/160, Loss: 5.272844425552641\n",
            "Epoch 63/160, Loss: 5.273014130799667\n",
            "Epoch 64/160, Loss: 5.2696897599398325\n",
            "Epoch 65/160, Loss: 5.269141367939122\n",
            "Epoch 66/160, Loss: 5.267739987434329\n",
            "Epoch 67/160, Loss: 5.262064038640093\n",
            "Epoch 68/160, Loss: 5.262710020060429\n",
            "Epoch 69/160, Loss: 5.26497571852506\n",
            "Epoch 70/160, Loss: 5.267924130725129\n",
            "Epoch 71/160, Loss: 5.263949810086614\n",
            "Epoch 72/160, Loss: 5.263439202857445\n",
            "Epoch 73/160, Loss: 5.260375257038399\n",
            "Epoch 74/160, Loss: 5.256341828104785\n",
            "Epoch 75/160, Loss: 5.268330110613342\n",
            "Epoch 76/160, Loss: 5.257820319641581\n",
            "Epoch 77/160, Loss: 5.26665345848064\n",
            "Epoch 78/160, Loss: 5.2554745308273585\n",
            "Epoch 79/160, Loss: 5.254669749218484\n",
            "Epoch 80/160, Loss: 5.248428820344189\n",
            "Epoch 81/160, Loss: 5.262423022628745\n",
            "Epoch 82/160, Loss: 5.253423921287517\n",
            "Epoch 83/160, Loss: 5.250509026715212\n",
            "Epoch 84/160, Loss: 5.251403346390981\n",
            "Epoch 85/160, Loss: 5.24808494148352\n",
            "Epoch 86/160, Loss: 5.258055561338849\n",
            "Epoch 87/160, Loss: 5.253638228492054\n",
            "Epoch 88/160, Loss: 5.247930179166672\n",
            "Epoch 89/160, Loss: 5.254049704812677\n",
            "Epoch 90/160, Loss: 5.253502251851894\n",
            "Epoch 91/160, Loss: 5.255874891110393\n",
            "Epoch 92/160, Loss: 5.245214684235165\n",
            "Epoch 93/160, Loss: 5.254409242469027\n",
            "Epoch 94/160, Loss: 5.252878092743856\n",
            "Epoch 95/160, Loss: 5.2474452126056645\n",
            "Epoch 96/160, Loss: 5.252705211834529\n",
            "Epoch 97/160, Loss: 5.249207651523678\n",
            "Epoch 98/160, Loss: 5.244304138681163\n",
            "Epoch 99/160, Loss: 5.2433456240407645\n",
            "Epoch 100/160, Loss: 5.253075500888288\n",
            "Epoch 101/160, Loss: 5.238629604544481\n",
            "Epoch 102/160, Loss: 5.167321649048945\n",
            "Epoch 103/160, Loss: 5.096407313481011\n",
            "Epoch 104/160, Loss: 5.064060895339303\n",
            "Epoch 105/160, Loss: 5.042443994975761\n",
            "Epoch 106/160, Loss: 5.020972454334464\n",
            "Epoch 107/160, Loss: 5.0166107516764376\n",
            "Epoch 108/160, Loss: 5.013605761710945\n",
            "Epoch 109/160, Loss: 4.995260110596561\n",
            "Epoch 110/160, Loss: 4.986324413962986\n",
            "Epoch 111/160, Loss: 4.975566954259068\n",
            "Epoch 112/160, Loss: 4.990300993175458\n",
            "Epoch 113/160, Loss: 4.969642596476523\n",
            "Epoch 114/160, Loss: 4.965917293372971\n",
            "Epoch 115/160, Loss: 4.960640604843569\n",
            "Epoch 116/160, Loss: 4.968510252130611\n",
            "Epoch 117/160, Loss: 4.962597142095151\n",
            "Epoch 118/160, Loss: 4.955325124208884\n",
            "Epoch 119/160, Loss: 4.954682050153727\n",
            "Epoch 120/160, Loss: 4.952639316353957\n",
            "Epoch 121/160, Loss: 4.943622709845033\n",
            "Epoch 122/160, Loss: 4.9431586302149935\n",
            "Epoch 123/160, Loss: 4.941291462734837\n",
            "Epoch 124/160, Loss: 4.938611814737929\n",
            "Epoch 125/160, Loss: 4.929049626030885\n",
            "Epoch 126/160, Loss: 4.867685928978883\n",
            "Epoch 127/160, Loss: 4.84298396049558\n",
            "Epoch 128/160, Loss: 4.790827271273679\n",
            "Epoch 129/160, Loss: 4.77598700316056\n",
            "Epoch 130/160, Loss: 4.753234442542581\n",
            "Epoch 131/160, Loss: 4.7408141071534216\n",
            "Epoch 132/160, Loss: 4.727585205336666\n",
            "Epoch 133/160, Loss: 4.723704298438928\n",
            "Epoch 134/160, Loss: 4.717121225488765\n",
            "Epoch 135/160, Loss: 4.709707863190594\n",
            "Epoch 136/160, Loss: 4.707402018329981\n",
            "Epoch 137/160, Loss: 4.697398244267535\n",
            "Epoch 138/160, Loss: 4.697793959351757\n",
            "Epoch 139/160, Loss: 4.691966365975187\n",
            "Epoch 140/160, Loss: 4.691377186104464\n",
            "Epoch 141/160, Loss: 4.683756844162026\n",
            "Epoch 142/160, Loss: 4.675369679165618\n",
            "Epoch 143/160, Loss: 4.680766677612539\n",
            "Epoch 144/160, Loss: 4.674802941129641\n",
            "Epoch 145/160, Loss: 4.659045534670505\n",
            "Epoch 146/160, Loss: 4.673514613714974\n",
            "Epoch 147/160, Loss: 4.660426513618216\n",
            "Epoch 148/160, Loss: 4.663359936545877\n",
            "Epoch 149/160, Loss: 4.662598916026942\n",
            "Epoch 150/160, Loss: 4.656229339902054\n",
            "Epoch 151/160, Loss: 4.656400856154654\n",
            "Epoch 152/160, Loss: 4.659065063652175\n",
            "Epoch 153/160, Loss: 4.653815920090737\n",
            "Epoch 154/160, Loss: 4.650636382724928\n",
            "Epoch 155/160, Loss: 4.6500489711761475\n",
            "Epoch 156/160, Loss: 4.648977326005316\n",
            "Epoch 157/160, Loss: 4.648194900254154\n",
            "Epoch 158/160, Loss: 4.649625308678278\n",
            "Epoch 159/160, Loss: 4.647877202924255\n",
            "Epoch 160/160, Loss: 4.638485957899362\n",
            "Test Accuracy: 31.34%\n",
            "Teacher accuracy: 52.15%\n",
            "Student accuracy without teacher: 26.77%\n",
            "Student accuracy with CE + KD: 31.34%\n"
          ]
        }
      ]
    }
  ]
}