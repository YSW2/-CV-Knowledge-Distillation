{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YSW2/CV-KnowledgeDistillation/blob/master/TAKD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "# Check if GPU is available, and if not, use the CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "sxUbUGdNbRXP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, epochs, learning_rate, device):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True, weight_decay=1e-4)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            # inputs: A collection of batch_size images\n",
        "            # labels: A vector of dimensionality batch_size with integers denoting class of each image\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # outputs: Output of the network for the collection of images. A tensor of dimensionality batch_size x num_classes\n",
        "            # labels: The actual labels of the images. Vector of dimensionality batch_size\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n",
        "\n",
        "def test(model, test_loader, device):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "bX2wiSCTgCAH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8mPg6Qu0af5p"
      },
      "outputs": [],
      "source": [
        "def data_loader(num_classes=10):\n",
        "  # Below we are preprocessing data for CIFAR-10. We use an arbitrary batch size of 128.\n",
        "  transforms_cifar = transforms.Compose([\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "  ])\n",
        "\n",
        "  if num_classes == 10:\n",
        "    # Loading the CIFAR-10 dataset:\n",
        "    train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms_cifar)\n",
        "    test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms_cifar)\n",
        "\n",
        "  elif num_classes == 100:\n",
        "    train_dataset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transforms_cifar)\n",
        "    test_dataset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transforms_cifar)\n",
        "\n",
        "  return train_dataset, test_dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvNetMaker(nn.Module):\n",
        "    \"\"\"\n",
        "    Creates a convolutional neural network (CNN) based on a given specification of layers.\n",
        "    \"\"\"\n",
        "    def __init__(self, layers):\n",
        "        \"\"\"\n",
        "        Initializes the CNN model with the specified layers.\n",
        "\n",
        "        :param layers: A list of strings, each representing a layer specification,\n",
        "                       such as [\"Conv64\", \"MaxPool\", \"Conv128\", \"MaxPool\", \"FC100\", \"FC10\"].\n",
        "                       \"Conv64\" means a convolutional layer with 64 filters,\n",
        "                       \"MaxPool\" means a max pooling layer,\n",
        "                       \"FC100\" means a fully connected layer with 100 neurons.\n",
        "        \"\"\"\n",
        "        super(ConvNetMaker, self).__init__()\n",
        "        self.conv_layers = []\n",
        "        self.fc_layers = []\n",
        "        h, w, d = 32, 32, 3\n",
        "        previous_layer_filter_count = 3\n",
        "        previous_layer_size = h * w * d\n",
        "        num_fc_layers_remained = len([1 for l in layers if l.startswith(\"FC\")])\n",
        "        for layer in layers:\n",
        "            if layer.startswith(\"Conv\"):\n",
        "                filter_count = int(layer[4:])\n",
        "                self.conv_layers += [\n",
        "                    nn.Conv2d(\n",
        "                        previous_layer_filter_count,\n",
        "                        filter_count,\n",
        "                        kernel_size=3,\n",
        "                        padding=1,\n",
        "                    ),\n",
        "                    nn.BatchNorm2d(filter_count),\n",
        "                    nn.ReLU(inplace=True),\n",
        "                ]\n",
        "                previous_layer_filter_count = filter_count\n",
        "                d = filter_count\n",
        "                previous_layer_size = h * w * d\n",
        "            elif layer.startswith(\"MaxPool\"):\n",
        "                self.conv_layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "                h, w = int(h / 2.0), int(w / 2.0)\n",
        "                previous_layer_size = h * w * d\n",
        "            elif layer.startswith(\"FC\"):\n",
        "                num_fc_layers_remained -= 1\n",
        "                current_layer_size = int(layer[2:])\n",
        "                if num_fc_layers_remained == 0:\n",
        "                    self.fc_layers += [\n",
        "                        nn.Linear(previous_layer_size, current_layer_size)\n",
        "                    ]\n",
        "                else:\n",
        "                    self.fc_layers += [\n",
        "                        nn.Linear(previous_layer_size, current_layer_size),\n",
        "                        nn.ReLU(inplace=True),\n",
        "                    ]\n",
        "                previous_layer_size = current_layer_size\n",
        "\n",
        "        conv_layers = self.conv_layers\n",
        "        fc_layers = self.fc_layers\n",
        "        self.conv_layers = nn.Sequential(*conv_layers)\n",
        "        self.fc_layers = nn.Sequential(*fc_layers)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Defines the forward pass of the model.\n",
        "\n",
        "        :param x: Input tensor of shape (batch_size, 3, 32, 32)\n",
        "        :return: Output tensor\n",
        "        \"\"\"\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc_layers(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "5Vta5updbKCH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plane_cifar10_book = {\n",
        "\t'2': ['Conv16', 'MaxPool', 'Conv16', 'MaxPool', 'FC10'],\n",
        "\t'4': ['Conv16', 'Conv16', 'MaxPool', 'Conv32', 'Conv32', 'MaxPool', 'FC10'],\n",
        "\t'6': ['Conv16', 'Conv16', 'MaxPool', 'Conv32', 'Conv32', 'MaxPool', 'Conv64', 'Conv64', 'MaxPool', 'FC10'],\n",
        "\t'8': ['Conv16', 'Conv16', 'MaxPool', 'Conv32', 'Conv32', 'MaxPool', 'Conv64', 'Conv64', 'MaxPool',\n",
        "\t\t  'Conv128', 'Conv128','MaxPool', 'FC64', 'FC10'],\n",
        "\t'10': ['Conv32', 'Conv32', 'MaxPool', 'Conv64', 'Conv64', 'MaxPool', 'Conv128', 'Conv128', 'MaxPool',\n",
        "\t\t   'Conv256', 'Conv256', 'Conv256', 'Conv256' , 'MaxPool', 'FC128' ,'FC10'],\n",
        "}\n",
        "\n",
        "\n",
        "plane_cifar100_book = {\n",
        "\t'2': ['Conv32', 'MaxPool', 'Conv32', 'MaxPool', 'FC100'],\n",
        "\t'4': ['Conv32', 'Conv32', 'MaxPool', 'Conv64', 'Conv64', 'MaxPool', 'FC100'],\n",
        "\t'6': ['Conv32', 'Conv32', 'MaxPool', 'Conv64', 'Conv64', 'MaxPool','Conv128', 'Conv128' ,'FC100'],\n",
        "\t'8': ['Conv32', 'Conv32', 'MaxPool', 'Conv64', 'Conv64', 'MaxPool', 'Conv128', 'Conv128', 'MaxPool',\n",
        "\t\t  'Conv256', 'Conv256','MaxPool', 'FC64', 'FC100'],\n",
        "\t'10': ['Conv32', 'Conv32', 'MaxPool', 'Conv64', 'Conv64', 'MaxPool', 'Conv128', 'Conv128', 'MaxPool',\n",
        "\t\t   'Conv256', 'Conv256', 'Conv256', 'Conv256' , 'MaxPool', 'FC512', 'FC100'],\n",
        "}"
      ],
      "metadata": {
        "id": "NloxAxsqhl8c"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dataloaders\n",
        "train_dataset, test_dataset = data_loader(100)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yg1ENrEngwbm",
        "outputId": "d19ac5fb-93a8-4040-8d32-d3072821b3fb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169001437/169001437 [00:02<00:00, 58360387.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-100-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "layer = plane_cifar100_book.get('10')\n",
        "\n",
        "teacher_model = ConvNetMaker(layer).to(device)\n",
        "train(teacher_model, train_loader, epochs=160, learning_rate=0.1, device=device)\n",
        "test_accuracy_deep = test(teacher_model, test_loader, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BagGMs5EgJ7i",
        "outputId": "16b8b711-5215-40b7-d812-3d62dc142a7c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/160, Loss: 4.003219184973051\n",
            "Epoch 2/160, Loss: 3.3952798044590082\n",
            "Epoch 3/160, Loss: 2.8476629019393336\n",
            "Epoch 4/160, Loss: 2.4452239979258583\n",
            "Epoch 5/160, Loss: 2.15524302785049\n",
            "Epoch 6/160, Loss: 1.9290856101628764\n",
            "Epoch 7/160, Loss: 1.7430062915967859\n",
            "Epoch 8/160, Loss: 1.5813551598497668\n",
            "Epoch 9/160, Loss: 1.4427197171599053\n",
            "Epoch 10/160, Loss: 1.3058164782841186\n",
            "Epoch 11/160, Loss: 1.183266849773924\n",
            "Epoch 12/160, Loss: 1.0882172069281264\n",
            "Epoch 13/160, Loss: 0.9904938300552271\n",
            "Epoch 14/160, Loss: 0.8914372850866878\n",
            "Epoch 15/160, Loss: 0.8160168010255565\n",
            "Epoch 16/160, Loss: 0.7422311014074194\n",
            "Epoch 17/160, Loss: 0.6894257880385269\n",
            "Epoch 18/160, Loss: 0.621968527812787\n",
            "Epoch 19/160, Loss: 0.5739047907654892\n",
            "Epoch 20/160, Loss: 0.5311827224386317\n",
            "Epoch 21/160, Loss: 0.5074615631719379\n",
            "Epoch 22/160, Loss: 0.4667114642498743\n",
            "Epoch 23/160, Loss: 0.43730776191062637\n",
            "Epoch 24/160, Loss: 0.40425234019298995\n",
            "Epoch 25/160, Loss: 0.39917344864829424\n",
            "Epoch 26/160, Loss: 0.3683298610512863\n",
            "Epoch 27/160, Loss: 0.3487804871233528\n",
            "Epoch 28/160, Loss: 0.3376535242971252\n",
            "Epoch 29/160, Loss: 0.3221834798145782\n",
            "Epoch 30/160, Loss: 0.2996059529616705\n",
            "Epoch 31/160, Loss: 0.2907462551084626\n",
            "Epoch 32/160, Loss: 0.28978885721672526\n",
            "Epoch 33/160, Loss: 0.29482974117750405\n",
            "Epoch 34/160, Loss: 0.2802515411963853\n",
            "Epoch 35/160, Loss: 0.26847782823473904\n",
            "Epoch 36/160, Loss: 0.261263775875044\n",
            "Epoch 37/160, Loss: 0.25815378381010823\n",
            "Epoch 38/160, Loss: 0.25877124858100703\n",
            "Epoch 39/160, Loss: 0.24104218402177172\n",
            "Epoch 40/160, Loss: 0.23127723466176206\n",
            "Epoch 41/160, Loss: 0.2398661862477622\n",
            "Epoch 42/160, Loss: 0.23346588619605965\n",
            "Epoch 43/160, Loss: 0.22861977003495712\n",
            "Epoch 44/160, Loss: 0.24419622927370582\n",
            "Epoch 45/160, Loss: 0.22803646808161454\n",
            "Epoch 46/160, Loss: 0.24345323335750937\n",
            "Epoch 47/160, Loss: 0.2224844878210741\n",
            "Epoch 48/160, Loss: 0.2356867143107802\n",
            "Epoch 49/160, Loss: 0.2252775165621582\n",
            "Epoch 50/160, Loss: 0.2290450604179936\n",
            "Epoch 51/160, Loss: 0.21671721669833374\n",
            "Epoch 52/160, Loss: 0.21206963527233094\n",
            "Epoch 53/160, Loss: 0.21401575548798227\n",
            "Epoch 54/160, Loss: 0.2111930150891204\n",
            "Epoch 55/160, Loss: 0.21572450254960437\n",
            "Epoch 56/160, Loss: 0.21207385897026648\n",
            "Epoch 57/160, Loss: 0.21635698485176277\n",
            "Epoch 58/160, Loss: 0.2121013837778355\n",
            "Epoch 59/160, Loss: 0.2037882846696755\n",
            "Epoch 60/160, Loss: 0.20491063689141323\n",
            "Epoch 61/160, Loss: 0.21115393939492344\n",
            "Epoch 62/160, Loss: 0.2202261131914223\n",
            "Epoch 63/160, Loss: 0.21240092706306815\n",
            "Epoch 64/160, Loss: 0.20679065811893213\n",
            "Epoch 65/160, Loss: 0.21175943709471645\n",
            "Epoch 66/160, Loss: 0.20178137108912247\n",
            "Epoch 67/160, Loss: 0.18855829561686577\n",
            "Epoch 68/160, Loss: 0.19783449574085452\n",
            "Epoch 69/160, Loss: 0.2142303622211032\n",
            "Epoch 70/160, Loss: 0.19643201613250902\n",
            "Epoch 71/160, Loss: 0.18855165700663995\n",
            "Epoch 72/160, Loss: 0.19992331911802597\n",
            "Epoch 73/160, Loss: 0.2110948342725139\n",
            "Epoch 74/160, Loss: 0.19636397300016545\n",
            "Epoch 75/160, Loss: 0.1823663630182176\n",
            "Epoch 76/160, Loss: 0.1866913729101953\n",
            "Epoch 77/160, Loss: 0.18978490439407966\n",
            "Epoch 78/160, Loss: 0.2003257170395778\n",
            "Epoch 79/160, Loss: 0.2109904920067781\n",
            "Epoch 80/160, Loss: 0.2108622421712979\n",
            "Epoch 81/160, Loss: 0.20569917734931498\n",
            "Epoch 82/160, Loss: 0.18878694935261137\n",
            "Epoch 83/160, Loss: 0.18742415272747465\n",
            "Epoch 84/160, Loss: 0.17665506038062104\n",
            "Epoch 85/160, Loss: 0.19878057271356472\n",
            "Epoch 86/160, Loss: 0.18150387076503785\n",
            "Epoch 87/160, Loss: 0.18021522256571923\n",
            "Epoch 88/160, Loss: 0.191285424730014\n",
            "Epoch 89/160, Loss: 0.1968748270036162\n",
            "Epoch 90/160, Loss: 0.17881992732739205\n",
            "Epoch 91/160, Loss: 0.18697123045621017\n",
            "Epoch 92/160, Loss: 0.19320011061742481\n",
            "Epoch 93/160, Loss: 0.18935789875781445\n",
            "Epoch 94/160, Loss: 0.17299438498514083\n",
            "Epoch 95/160, Loss: 0.18878115831738543\n",
            "Epoch 96/160, Loss: 0.18236232363163968\n",
            "Epoch 97/160, Loss: 0.19561635556123447\n",
            "Epoch 98/160, Loss: 0.18593085548647528\n",
            "Epoch 99/160, Loss: 0.18457533589676214\n",
            "Epoch 100/160, Loss: 0.17647287617330357\n",
            "Epoch 101/160, Loss: 0.17804274693741212\n",
            "Epoch 102/160, Loss: 0.1828107048502511\n",
            "Epoch 103/160, Loss: 0.19032490380165523\n",
            "Epoch 104/160, Loss: 0.18971089755787568\n",
            "Epoch 105/160, Loss: 0.17798825241911137\n",
            "Epoch 106/160, Loss: 0.17514325302484854\n",
            "Epoch 107/160, Loss: 0.1647626356986325\n",
            "Epoch 108/160, Loss: 0.18178005512718046\n",
            "Epoch 109/160, Loss: 0.17024563541135673\n",
            "Epoch 110/160, Loss: 0.17876747259131784\n",
            "Epoch 111/160, Loss: 0.18872296651039283\n",
            "Epoch 112/160, Loss: 0.1862892969166074\n",
            "Epoch 113/160, Loss: 0.17616564291708\n",
            "Epoch 114/160, Loss: 0.18794930001239643\n",
            "Epoch 115/160, Loss: 0.17366412387746374\n",
            "Epoch 116/160, Loss: 0.17794803309890314\n",
            "Epoch 117/160, Loss: 0.17654606723286154\n",
            "Epoch 118/160, Loss: 0.18034242329847477\n",
            "Epoch 119/160, Loss: 0.17954906708825275\n",
            "Epoch 120/160, Loss: 0.1707569175230725\n",
            "Epoch 121/160, Loss: 0.18735236381097217\n",
            "Epoch 122/160, Loss: 0.1780316882416644\n",
            "Epoch 123/160, Loss: 0.1648346551734468\n",
            "Epoch 124/160, Loss: 0.16499393246115168\n",
            "Epoch 125/160, Loss: 0.16623019583313667\n",
            "Epoch 126/160, Loss: 0.1708837843612027\n",
            "Epoch 127/160, Loss: 0.16268291857922473\n",
            "Epoch 128/160, Loss: 0.16927780553488933\n",
            "Epoch 129/160, Loss: 0.1791324611000545\n",
            "Epoch 130/160, Loss: 0.17546405215435626\n",
            "Epoch 131/160, Loss: 0.16891243394530947\n",
            "Epoch 132/160, Loss: 0.17230739744613544\n",
            "Epoch 133/160, Loss: 0.17724814493675975\n",
            "Epoch 134/160, Loss: 0.1724941772134865\n",
            "Epoch 135/160, Loss: 0.1662350160062618\n",
            "Epoch 136/160, Loss: 0.1699807967018822\n",
            "Epoch 137/160, Loss: 0.1692961721688204\n",
            "Epoch 138/160, Loss: 0.16710390790801524\n",
            "Epoch 139/160, Loss: 0.16976794591911917\n",
            "Epoch 140/160, Loss: 0.1780148827092117\n",
            "Epoch 141/160, Loss: 0.17811032526595208\n",
            "Epoch 142/160, Loss: 0.16709623714465924\n",
            "Epoch 143/160, Loss: 0.16403823761302797\n",
            "Epoch 144/160, Loss: 0.17216833614174973\n",
            "Epoch 145/160, Loss: 0.16819950438978726\n",
            "Epoch 146/160, Loss: 0.15915705971515087\n",
            "Epoch 147/160, Loss: 0.15992524258106414\n",
            "Epoch 148/160, Loss: 0.15877160606215068\n",
            "Epoch 149/160, Loss: 0.1710462174128236\n",
            "Epoch 150/160, Loss: 0.16741262905090057\n",
            "Epoch 151/160, Loss: 0.16065037245755\n",
            "Epoch 152/160, Loss: 0.16897471058551614\n",
            "Epoch 153/160, Loss: 0.17803108101458195\n",
            "Epoch 154/160, Loss: 0.17300528350769712\n",
            "Epoch 155/160, Loss: 0.17307680422235328\n",
            "Epoch 156/160, Loss: 0.15970195769368078\n",
            "Epoch 157/160, Loss: 0.1486682219673758\n",
            "Epoch 158/160, Loss: 0.16076627057379164\n",
            "Epoch 159/160, Loss: 0.16102540609248153\n",
            "Epoch 160/160, Loss: 0.1740321275656638\n",
            "Test Accuracy: 51.08%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "layer = plane_cifar100_book.get('2')\n",
        "torch.manual_seed(42)\n",
        "student_model = ConvNetMaker(layer).to(device)\n",
        "\n",
        "train(student_model, train_loader, epochs=160, learning_rate=0.1, device=device)\n",
        "test_accuracy_light_ce = test(student_model, test_loader, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5haIDBJ2kIp5",
        "outputId": "62c3e993-a9e2-44ff-a545-245c1ef2661c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/160, Loss: 4.239775285696434\n",
            "Epoch 2/160, Loss: 3.8019423356751347\n",
            "Epoch 3/160, Loss: 3.651276835395247\n",
            "Epoch 4/160, Loss: 3.5618112514086087\n",
            "Epoch 5/160, Loss: 3.489645529281148\n",
            "Epoch 6/160, Loss: 3.4355800554270632\n",
            "Epoch 7/160, Loss: 3.389972392860276\n",
            "Epoch 8/160, Loss: 3.3560808839090646\n",
            "Epoch 9/160, Loss: 3.3215228322216923\n",
            "Epoch 10/160, Loss: 3.2897250896219705\n",
            "Epoch 11/160, Loss: 3.270614765489193\n",
            "Epoch 12/160, Loss: 3.250949508393817\n",
            "Epoch 13/160, Loss: 3.2386700344817414\n",
            "Epoch 14/160, Loss: 3.220334542072033\n",
            "Epoch 15/160, Loss: 3.2080346989204815\n",
            "Epoch 16/160, Loss: 3.1948150753060265\n",
            "Epoch 17/160, Loss: 3.1832464399849973\n",
            "Epoch 18/160, Loss: 3.173683879930345\n",
            "Epoch 19/160, Loss: 3.165260699094104\n",
            "Epoch 20/160, Loss: 3.1542178944248676\n",
            "Epoch 21/160, Loss: 3.1503302953432284\n",
            "Epoch 22/160, Loss: 3.14686003060597\n",
            "Epoch 23/160, Loss: 3.137268679221268\n",
            "Epoch 24/160, Loss: 3.1337230327489127\n",
            "Epoch 25/160, Loss: 3.12435141731711\n",
            "Epoch 26/160, Loss: 3.1241686295365434\n",
            "Epoch 27/160, Loss: 3.1239989358750755\n",
            "Epoch 28/160, Loss: 3.1163922813542357\n",
            "Epoch 29/160, Loss: 3.114788495671109\n",
            "Epoch 30/160, Loss: 3.1098517871573756\n",
            "Epoch 31/160, Loss: 3.106331974039297\n",
            "Epoch 32/160, Loss: 3.103819150144182\n",
            "Epoch 33/160, Loss: 3.104112424508995\n",
            "Epoch 34/160, Loss: 3.0998243492887454\n",
            "Epoch 35/160, Loss: 3.0955489145215513\n",
            "Epoch 36/160, Loss: 3.091472975128447\n",
            "Epoch 37/160, Loss: 3.0906027034115606\n",
            "Epoch 38/160, Loss: 3.0914458033373897\n",
            "Epoch 39/160, Loss: 3.0866317035597\n",
            "Epoch 40/160, Loss: 3.0840936158319265\n",
            "Epoch 41/160, Loss: 3.087006851840202\n",
            "Epoch 42/160, Loss: 3.083589137362702\n",
            "Epoch 43/160, Loss: 3.0846515598199558\n",
            "Epoch 44/160, Loss: 3.0792110326040127\n",
            "Epoch 45/160, Loss: 3.077279615890035\n",
            "Epoch 46/160, Loss: 3.0763257312042938\n",
            "Epoch 47/160, Loss: 3.0744530587550014\n",
            "Epoch 48/160, Loss: 3.0712149405418456\n",
            "Epoch 49/160, Loss: 3.071326897882135\n",
            "Epoch 50/160, Loss: 3.0723226527728693\n",
            "Epoch 51/160, Loss: 3.0681685120858195\n",
            "Epoch 52/160, Loss: 3.0685851512967472\n",
            "Epoch 53/160, Loss: 3.074652968160332\n",
            "Epoch 54/160, Loss: 3.062877472709207\n",
            "Epoch 55/160, Loss: 3.063047591377707\n",
            "Epoch 56/160, Loss: 3.062704928390815\n",
            "Epoch 57/160, Loss: 3.060163883297035\n",
            "Epoch 58/160, Loss: 3.062171264072818\n",
            "Epoch 59/160, Loss: 3.063186882706859\n",
            "Epoch 60/160, Loss: 3.0576929082650968\n",
            "Epoch 61/160, Loss: 3.056344255462022\n",
            "Epoch 62/160, Loss: 3.061370721558476\n",
            "Epoch 63/160, Loss: 3.0566513568848905\n",
            "Epoch 64/160, Loss: 3.0566092372855262\n",
            "Epoch 65/160, Loss: 3.050663470612158\n",
            "Epoch 66/160, Loss: 3.056285575222786\n",
            "Epoch 67/160, Loss: 3.053895774704721\n",
            "Epoch 68/160, Loss: 3.051457647167508\n",
            "Epoch 69/160, Loss: 3.054078006378525\n",
            "Epoch 70/160, Loss: 3.048990454515228\n",
            "Epoch 71/160, Loss: 3.049623474135728\n",
            "Epoch 72/160, Loss: 3.0490358699008326\n",
            "Epoch 73/160, Loss: 3.050452471389185\n",
            "Epoch 74/160, Loss: 3.0464443020198657\n",
            "Epoch 75/160, Loss: 3.0439601609164186\n",
            "Epoch 76/160, Loss: 3.043490579366074\n",
            "Epoch 77/160, Loss: 3.0438845627143256\n",
            "Epoch 78/160, Loss: 3.0475317919650653\n",
            "Epoch 79/160, Loss: 3.041749081038453\n",
            "Epoch 80/160, Loss: 3.0472630155665796\n",
            "Epoch 81/160, Loss: 3.0413329095181907\n",
            "Epoch 82/160, Loss: 3.0386006393091147\n",
            "Epoch 83/160, Loss: 3.0416005785812805\n",
            "Epoch 84/160, Loss: 3.0362121107633158\n",
            "Epoch 85/160, Loss: 3.040771252663849\n",
            "Epoch 86/160, Loss: 3.0368610647938135\n",
            "Epoch 87/160, Loss: 3.041404649729619\n",
            "Epoch 88/160, Loss: 3.0386899334695334\n",
            "Epoch 89/160, Loss: 3.04096582234668\n",
            "Epoch 90/160, Loss: 3.035467302097994\n",
            "Epoch 91/160, Loss: 3.0379774119238108\n",
            "Epoch 92/160, Loss: 3.03514947915626\n",
            "Epoch 93/160, Loss: 3.0324856314207893\n",
            "Epoch 94/160, Loss: 3.034196634731634\n",
            "Epoch 95/160, Loss: 3.0373990133290403\n",
            "Epoch 96/160, Loss: 3.0328244986131674\n",
            "Epoch 97/160, Loss: 3.0315209532637732\n",
            "Epoch 98/160, Loss: 3.0341459058434763\n",
            "Epoch 99/160, Loss: 3.032800731146732\n",
            "Epoch 100/160, Loss: 3.031607558965073\n",
            "Epoch 101/160, Loss: 3.032792320031949\n",
            "Epoch 102/160, Loss: 3.0318702572142073\n",
            "Epoch 103/160, Loss: 3.0305455956617586\n",
            "Epoch 104/160, Loss: 3.0203359633150613\n",
            "Epoch 105/160, Loss: 2.957491150292594\n",
            "Epoch 106/160, Loss: 2.902548268018171\n",
            "Epoch 107/160, Loss: 2.8695754139014826\n",
            "Epoch 108/160, Loss: 2.8550719495319647\n",
            "Epoch 109/160, Loss: 2.842036639333076\n",
            "Epoch 110/160, Loss: 2.834571065195381\n",
            "Epoch 111/160, Loss: 2.822880780910287\n",
            "Epoch 112/160, Loss: 2.815020471582632\n",
            "Epoch 113/160, Loss: 2.814486780434923\n",
            "Epoch 114/160, Loss: 2.808525976317618\n",
            "Epoch 115/160, Loss: 2.806015671976387\n",
            "Epoch 116/160, Loss: 2.8038727706655515\n",
            "Epoch 117/160, Loss: 2.7991716928799133\n",
            "Epoch 118/160, Loss: 2.792942919084788\n",
            "Epoch 119/160, Loss: 2.793114475581957\n",
            "Epoch 120/160, Loss: 2.787324090747882\n",
            "Epoch 121/160, Loss: 2.790440131331344\n",
            "Epoch 122/160, Loss: 2.783805688018994\n",
            "Epoch 123/160, Loss: 2.7863003555161265\n",
            "Epoch 124/160, Loss: 2.7845418812978604\n",
            "Epoch 125/160, Loss: 2.78170066904229\n",
            "Epoch 126/160, Loss: 2.7798094822622628\n",
            "Epoch 127/160, Loss: 2.778625818164757\n",
            "Epoch 128/160, Loss: 2.777354128830268\n",
            "Epoch 129/160, Loss: 2.7734044890879366\n",
            "Epoch 130/160, Loss: 2.774318361526255\n",
            "Epoch 131/160, Loss: 2.7713727920561495\n",
            "Epoch 132/160, Loss: 2.7710834269023614\n",
            "Epoch 133/160, Loss: 2.768069895941888\n",
            "Epoch 134/160, Loss: 2.762904840357163\n",
            "Epoch 135/160, Loss: 2.7675017243456046\n",
            "Epoch 136/160, Loss: 2.763729374731898\n",
            "Epoch 137/160, Loss: 2.7678893228321124\n",
            "Epoch 138/160, Loss: 2.770208482547184\n",
            "Epoch 139/160, Loss: 2.7636861459678395\n",
            "Epoch 140/160, Loss: 2.760574476188406\n",
            "Epoch 141/160, Loss: 2.7624542164375714\n",
            "Epoch 142/160, Loss: 2.764258160920399\n",
            "Epoch 143/160, Loss: 2.7535860087255686\n",
            "Epoch 144/160, Loss: 2.7577794854293396\n",
            "Epoch 145/160, Loss: 2.752584347029781\n",
            "Epoch 146/160, Loss: 2.7607984426991106\n",
            "Epoch 147/160, Loss: 2.754258785711225\n",
            "Epoch 148/160, Loss: 2.7540970671817164\n",
            "Epoch 149/160, Loss: 2.7501567721062\n",
            "Epoch 150/160, Loss: 2.748915130829872\n",
            "Epoch 151/160, Loss: 2.7500479282320613\n",
            "Epoch 152/160, Loss: 2.749798836305623\n",
            "Epoch 153/160, Loss: 2.75003498960334\n",
            "Epoch 154/160, Loss: 2.746406742983767\n",
            "Epoch 155/160, Loss: 2.748268573180489\n",
            "Epoch 156/160, Loss: 2.75077197070012\n",
            "Epoch 157/160, Loss: 2.742710656827063\n",
            "Epoch 158/160, Loss: 2.7484924805438733\n",
            "Epoch 159/160, Loss: 2.7402573735512736\n",
            "Epoch 160/160, Loss: 2.7472267955770273\n",
            "Test Accuracy: 26.77%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_knowledge_distillation(teacher, student, train_loader, epochs, learning_rate, T, soft_target_loss_weight, ce_loss_weight, device):\n",
        "    ce_loss = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(student.parameters(), lr=learning_rate, momentum=0.9, nesterov=True, weight_decay=1e-4)\n",
        "\n",
        "    teacher.eval()  # Teacher set to evaluation mode\n",
        "    student.train() # Student to train mode\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass with the teacher model - do not save gradients here as we do not change the teacher's weights\n",
        "            with torch.no_grad():\n",
        "                teacher_logits = teacher(inputs)\n",
        "\n",
        "            # Forward pass with the student model\n",
        "            student_logits = student(inputs)\n",
        "\n",
        "            #Soften the student logits by applying softmax first and log() second\n",
        "            soft_targets = nn.functional.softmax(teacher_logits / T, dim=-1)\n",
        "            soft_prob = nn.functional.log_softmax(student_logits / T, dim=-1)\n",
        "\n",
        "            # Calculate the soft targets loss. Scaled by T**2 as suggested by the authors of the paper \"Distilling the knowledge in a neural network\"\n",
        "            soft_targets_loss = -torch.sum(soft_targets * soft_prob) / soft_prob.size()[0] * (T**2)\n",
        "\n",
        "            # Calculate the true label loss\n",
        "            label_loss = ce_loss(student_logits, labels)\n",
        "\n",
        "            # Weighted sum of the two losses\n",
        "            loss = soft_target_loss_weight * soft_targets_loss + ce_loss_weight * label_loss\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n"
      ],
      "metadata": {
        "id": "GWszARHJBIZL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply ``train_knowledge_distillation`` with a temperature of 2. Arbitrarily set the weights to 0.75 for CE and 0.25 for distillation loss.\n",
        "kd_student_model = ConvNetMaker(layer).to(device)\n",
        "train_knowledge_distillation(teacher=teacher_model, student=kd_student_model, train_loader=train_loader, epochs=160, learning_rate=0.1, T=2, soft_target_loss_weight=0.25, ce_loss_weight=0.75, device=device)\n",
        "test_accuracy_light_ce_and_kd = test(kd_student_model, test_loader, device)\n",
        "\n",
        "# Compare the student test accuracy with and without the teacher, after distillation\n",
        "print(f\"Teacher accuracy: {test_accuracy_deep:.2f}%\")\n",
        "print(f\"Student accuracy without teacher: {test_accuracy_light_ce:.2f}%\")\n",
        "print(f\"Student accuracy with CE + KD: {test_accuracy_light_ce_and_kd:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afIx7W2CBOCe",
        "outputId": "0e8b7827-56bf-42a0-a324-5ff3cdb00acb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/160, Loss: 7.300865008702973\n",
            "Epoch 2/160, Loss: 6.638971815328769\n",
            "Epoch 3/160, Loss: 6.403528075693818\n",
            "Epoch 4/160, Loss: 6.252380539389217\n",
            "Epoch 5/160, Loss: 6.163560833162664\n",
            "Epoch 6/160, Loss: 6.0971230923977044\n",
            "Epoch 7/160, Loss: 6.063027346530534\n",
            "Epoch 8/160, Loss: 6.0121019051203035\n",
            "Epoch 9/160, Loss: 5.930418932834245\n",
            "Epoch 10/160, Loss: 5.754262536383041\n",
            "Epoch 11/160, Loss: 5.649084627780768\n",
            "Epoch 12/160, Loss: 5.589612998620933\n",
            "Epoch 13/160, Loss: 5.548440801518042\n",
            "Epoch 14/160, Loss: 5.51570523791301\n",
            "Epoch 15/160, Loss: 5.484533177007495\n",
            "Epoch 16/160, Loss: 5.473247542710561\n",
            "Epoch 17/160, Loss: 5.453340045021623\n",
            "Epoch 18/160, Loss: 5.435664272064443\n",
            "Epoch 19/160, Loss: 5.42592137792836\n",
            "Epoch 20/160, Loss: 5.414791857190145\n",
            "Epoch 21/160, Loss: 5.400842664186912\n",
            "Epoch 22/160, Loss: 5.3958924664255905\n",
            "Epoch 23/160, Loss: 5.3903499930106165\n",
            "Epoch 24/160, Loss: 5.374655116244655\n",
            "Epoch 25/160, Loss: 5.376932133189248\n",
            "Epoch 26/160, Loss: 5.366078758483653\n",
            "Epoch 27/160, Loss: 5.366943319130431\n",
            "Epoch 28/160, Loss: 5.349156678485139\n",
            "Epoch 29/160, Loss: 5.346994350023587\n",
            "Epoch 30/160, Loss: 5.348904398701075\n",
            "Epoch 31/160, Loss: 5.341142784908909\n",
            "Epoch 32/160, Loss: 5.339903000980387\n",
            "Epoch 33/160, Loss: 5.337399060768849\n",
            "Epoch 34/160, Loss: 5.3371380344986\n",
            "Epoch 35/160, Loss: 5.326633933864896\n",
            "Epoch 36/160, Loss: 5.3231803255008\n",
            "Epoch 37/160, Loss: 5.328901825048734\n",
            "Epoch 38/160, Loss: 5.316043833027715\n",
            "Epoch 39/160, Loss: 5.316182191414601\n",
            "Epoch 40/160, Loss: 5.3120979470060306\n",
            "Epoch 41/160, Loss: 5.312575811010492\n",
            "Epoch 42/160, Loss: 5.30600295347326\n",
            "Epoch 43/160, Loss: 5.311285890886546\n",
            "Epoch 44/160, Loss: 5.303535212641177\n",
            "Epoch 45/160, Loss: 5.304250348864309\n",
            "Epoch 46/160, Loss: 5.301071259676648\n",
            "Epoch 47/160, Loss: 5.292406740700803\n",
            "Epoch 48/160, Loss: 5.300586438239993\n",
            "Epoch 49/160, Loss: 5.292142098517064\n",
            "Epoch 50/160, Loss: 5.3033924968651185\n",
            "Epoch 51/160, Loss: 5.2933882984054055\n",
            "Epoch 52/160, Loss: 5.284181693630755\n",
            "Epoch 53/160, Loss: 5.285444705992403\n",
            "Epoch 54/160, Loss: 5.287877693810426\n",
            "Epoch 55/160, Loss: 5.280944344942527\n",
            "Epoch 56/160, Loss: 5.2756119506133485\n",
            "Epoch 57/160, Loss: 5.281743617923668\n",
            "Epoch 58/160, Loss: 5.279913786427139\n",
            "Epoch 59/160, Loss: 5.275849024048242\n",
            "Epoch 60/160, Loss: 5.274100334138212\n",
            "Epoch 61/160, Loss: 5.276645467714276\n",
            "Epoch 62/160, Loss: 5.272844425552641\n",
            "Epoch 63/160, Loss: 5.273014130799667\n",
            "Epoch 64/160, Loss: 5.2696897599398325\n",
            "Epoch 65/160, Loss: 5.269141367939122\n",
            "Epoch 66/160, Loss: 5.267739987434329\n",
            "Epoch 67/160, Loss: 5.262064038640093\n",
            "Epoch 68/160, Loss: 5.262710020060429\n",
            "Epoch 69/160, Loss: 5.26497571852506\n",
            "Epoch 70/160, Loss: 5.267924130725129\n",
            "Epoch 71/160, Loss: 5.263949810086614\n",
            "Epoch 72/160, Loss: 5.263439202857445\n",
            "Epoch 73/160, Loss: 5.260375257038399\n",
            "Epoch 74/160, Loss: 5.256341828104785\n",
            "Epoch 75/160, Loss: 5.268330110613342\n",
            "Epoch 76/160, Loss: 5.257820319641581\n",
            "Epoch 77/160, Loss: 5.26665345848064\n",
            "Epoch 78/160, Loss: 5.2554745308273585\n",
            "Epoch 79/160, Loss: 5.254669749218484\n",
            "Epoch 80/160, Loss: 5.248428820344189\n",
            "Epoch 81/160, Loss: 5.262423022628745\n",
            "Epoch 82/160, Loss: 5.253423921287517\n",
            "Epoch 83/160, Loss: 5.250509026715212\n",
            "Epoch 84/160, Loss: 5.251403346390981\n",
            "Epoch 85/160, Loss: 5.24808494148352\n",
            "Epoch 86/160, Loss: 5.258055561338849\n",
            "Epoch 87/160, Loss: 5.253638228492054\n",
            "Epoch 88/160, Loss: 5.247930179166672\n",
            "Epoch 89/160, Loss: 5.254049704812677\n",
            "Epoch 90/160, Loss: 5.253502251851894\n",
            "Epoch 91/160, Loss: 5.255874891110393\n",
            "Epoch 92/160, Loss: 5.245214684235165\n",
            "Epoch 93/160, Loss: 5.254409242469027\n",
            "Epoch 94/160, Loss: 5.252878092743856\n",
            "Epoch 95/160, Loss: 5.2474452126056645\n",
            "Epoch 96/160, Loss: 5.252705211834529\n",
            "Epoch 97/160, Loss: 5.249207651523678\n",
            "Epoch 98/160, Loss: 5.244304138681163\n",
            "Epoch 99/160, Loss: 5.2433456240407645\n",
            "Epoch 100/160, Loss: 5.253075500888288\n",
            "Epoch 101/160, Loss: 5.238629604544481\n",
            "Epoch 102/160, Loss: 5.167321649048945\n",
            "Epoch 103/160, Loss: 5.096407313481011\n",
            "Epoch 104/160, Loss: 5.064060895339303\n",
            "Epoch 105/160, Loss: 5.042443994975761\n",
            "Epoch 106/160, Loss: 5.020972454334464\n",
            "Epoch 107/160, Loss: 5.0166107516764376\n",
            "Epoch 108/160, Loss: 5.013605761710945\n",
            "Epoch 109/160, Loss: 4.995260110596561\n",
            "Epoch 110/160, Loss: 4.986324413962986\n",
            "Epoch 111/160, Loss: 4.975566954259068\n",
            "Epoch 112/160, Loss: 4.990300993175458\n",
            "Epoch 113/160, Loss: 4.969642596476523\n",
            "Epoch 114/160, Loss: 4.965917293372971\n",
            "Epoch 115/160, Loss: 4.960640604843569\n",
            "Epoch 116/160, Loss: 4.968510252130611\n",
            "Epoch 117/160, Loss: 4.962597142095151\n",
            "Epoch 118/160, Loss: 4.955325124208884\n",
            "Epoch 119/160, Loss: 4.954682050153727\n",
            "Epoch 120/160, Loss: 4.952639316353957\n",
            "Epoch 121/160, Loss: 4.943622709845033\n",
            "Epoch 122/160, Loss: 4.9431586302149935\n",
            "Epoch 123/160, Loss: 4.941291462734837\n",
            "Epoch 124/160, Loss: 4.938611814737929\n",
            "Epoch 125/160, Loss: 4.929049626030885\n",
            "Epoch 126/160, Loss: 4.867685928978883\n",
            "Epoch 127/160, Loss: 4.84298396049558\n",
            "Epoch 128/160, Loss: 4.790827271273679\n",
            "Epoch 129/160, Loss: 4.77598700316056\n",
            "Epoch 130/160, Loss: 4.753234442542581\n",
            "Epoch 131/160, Loss: 4.7408141071534216\n",
            "Epoch 132/160, Loss: 4.727585205336666\n",
            "Epoch 133/160, Loss: 4.723704298438928\n",
            "Epoch 134/160, Loss: 4.717121225488765\n",
            "Epoch 135/160, Loss: 4.709707863190594\n",
            "Epoch 136/160, Loss: 4.707402018329981\n",
            "Epoch 137/160, Loss: 4.697398244267535\n",
            "Epoch 138/160, Loss: 4.697793959351757\n",
            "Epoch 139/160, Loss: 4.691966365975187\n",
            "Epoch 140/160, Loss: 4.691377186104464\n",
            "Epoch 141/160, Loss: 4.683756844162026\n",
            "Epoch 142/160, Loss: 4.675369679165618\n",
            "Epoch 143/160, Loss: 4.680766677612539\n",
            "Epoch 144/160, Loss: 4.674802941129641\n",
            "Epoch 145/160, Loss: 4.659045534670505\n",
            "Epoch 146/160, Loss: 4.673514613714974\n",
            "Epoch 147/160, Loss: 4.660426513618216\n",
            "Epoch 148/160, Loss: 4.663359936545877\n",
            "Epoch 149/160, Loss: 4.662598916026942\n",
            "Epoch 150/160, Loss: 4.656229339902054\n",
            "Epoch 151/160, Loss: 4.656400856154654\n",
            "Epoch 152/160, Loss: 4.659065063652175\n",
            "Epoch 153/160, Loss: 4.653815920090737\n",
            "Epoch 154/160, Loss: 4.650636382724928\n",
            "Epoch 155/160, Loss: 4.6500489711761475\n",
            "Epoch 156/160, Loss: 4.648977326005316\n",
            "Epoch 157/160, Loss: 4.648194900254154\n",
            "Epoch 158/160, Loss: 4.649625308678278\n",
            "Epoch 159/160, Loss: 4.647877202924255\n",
            "Epoch 160/160, Loss: 4.638485957899362\n",
            "Test Accuracy: 31.34%\n",
            "Teacher accuracy: 52.15%\n",
            "Student accuracy without teacher: 26.77%\n",
            "Student accuracy with CE + KD: 31.34%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "layer = plane_cifar100_book.get('4')\n",
        "torch.manual_seed(42)\n",
        "TA_model = ConvNetMaker(layer).to(device)\n",
        "\n",
        "layer = plane_cifar100_book.get('2')\n",
        "torch.manual_seed(42)\n",
        "takd_student_model = ConvNetMaker(layer).to(device)\n",
        "\n",
        "train_knowledge_distillation(teacher=teacher_model, student=TA_model, train_loader=train_loader, epochs=160, learning_rate=0.1, T=2, soft_target_loss_weight=0.25, ce_loss_weight=0.75, device=device)\n",
        "train_knowledge_distillation(teacher=TA_model, student=takd_student_model, train_loader=train_loader, epochs=160, learning_rate=0.1, T=2, soft_target_loss_weight=0.25, ce_loss_weight=0.75, device=device)\n",
        "\n",
        "test_accuracy_light_ce_and_kd = test(kd_student_model, test_loader, device)"
      ],
      "metadata": {
        "id": "oHJISaNYQBab",
        "outputId": "a8e3f425-cc84-4245-a6d9-3d16259e6812",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/160, Loss: 7.750999433610141\n",
            "Epoch 2/160, Loss: 7.184969680083682\n",
            "Epoch 3/160, Loss: 6.928907361481806\n",
            "Epoch 4/160, Loss: 6.670944330942295\n",
            "Epoch 5/160, Loss: 6.4661863685568886\n",
            "Epoch 6/160, Loss: 6.317317906547995\n",
            "Epoch 7/160, Loss: 6.212963865236248\n",
            "Epoch 8/160, Loss: 6.12069755320049\n",
            "Epoch 9/160, Loss: 6.031972547321368\n",
            "Epoch 10/160, Loss: 5.9532398567785085\n",
            "Epoch 11/160, Loss: 5.891038439767745\n",
            "Epoch 12/160, Loss: 5.776221681433871\n",
            "Epoch 13/160, Loss: 5.5960370707694835\n",
            "Epoch 14/160, Loss: 5.48133436432275\n",
            "Epoch 15/160, Loss: 5.38174731042379\n",
            "Epoch 16/160, Loss: 5.237235292449327\n",
            "Epoch 17/160, Loss: 5.124783202510356\n",
            "Epoch 18/160, Loss: 5.044443091468128\n",
            "Epoch 19/160, Loss: 4.967119112039161\n",
            "Epoch 20/160, Loss: 4.907138307381164\n",
            "Epoch 21/160, Loss: 4.854494366804352\n",
            "Epoch 22/160, Loss: 4.814559474320668\n",
            "Epoch 23/160, Loss: 4.775302980866884\n",
            "Epoch 24/160, Loss: 4.738764893368382\n",
            "Epoch 25/160, Loss: 4.701769859284696\n",
            "Epoch 26/160, Loss: 4.669769956632648\n",
            "Epoch 27/160, Loss: 4.645368311411279\n",
            "Epoch 28/160, Loss: 4.620711093058672\n",
            "Epoch 29/160, Loss: 4.591895251018007\n",
            "Epoch 30/160, Loss: 4.530678263100822\n",
            "Epoch 31/160, Loss: 4.440901557502844\n",
            "Epoch 32/160, Loss: 4.358165612915897\n",
            "Epoch 33/160, Loss: 4.28943439273883\n",
            "Epoch 34/160, Loss: 4.203642490879654\n",
            "Epoch 35/160, Loss: 4.150276501770215\n",
            "Epoch 36/160, Loss: 4.093973443941082\n",
            "Epoch 37/160, Loss: 4.041694865507238\n",
            "Epoch 38/160, Loss: 3.9804447996037085\n",
            "Epoch 39/160, Loss: 3.925033126645686\n",
            "Epoch 40/160, Loss: 3.8747015316468065\n",
            "Epoch 41/160, Loss: 3.8195703121097497\n",
            "Epoch 42/160, Loss: 3.769041060791601\n",
            "Epoch 43/160, Loss: 3.7312008834555934\n",
            "Epoch 44/160, Loss: 3.691909495521994\n",
            "Epoch 45/160, Loss: 3.654058246051564\n",
            "Epoch 46/160, Loss: 3.6225941547042573\n",
            "Epoch 47/160, Loss: 3.595585379759064\n",
            "Epoch 48/160, Loss: 3.56731134119546\n",
            "Epoch 49/160, Loss: 3.528796852092304\n",
            "Epoch 50/160, Loss: 3.4985954785895776\n",
            "Epoch 51/160, Loss: 3.4578274021977964\n",
            "Epoch 52/160, Loss: 3.424959552867333\n",
            "Epoch 53/160, Loss: 3.406828062003836\n",
            "Epoch 54/160, Loss: 3.378981655515978\n",
            "Epoch 55/160, Loss: 3.3527027414277994\n",
            "Epoch 56/160, Loss: 3.3387081269413006\n",
            "Epoch 57/160, Loss: 3.3173717842687425\n",
            "Epoch 58/160, Loss: 3.3065878735173997\n",
            "Epoch 59/160, Loss: 3.284219548525408\n",
            "Epoch 60/160, Loss: 3.2637931507871585\n",
            "Epoch 61/160, Loss: 3.251479223256221\n",
            "Epoch 62/160, Loss: 3.2433732909619657\n",
            "Epoch 63/160, Loss: 3.2236533841818495\n",
            "Epoch 64/160, Loss: 3.2207339779495276\n",
            "Epoch 65/160, Loss: 3.2006566122059934\n",
            "Epoch 66/160, Loss: 3.194543996430419\n",
            "Epoch 67/160, Loss: 3.177237203358994\n",
            "Epoch 68/160, Loss: 3.165875717197233\n",
            "Epoch 69/160, Loss: 3.165756459736153\n",
            "Epoch 70/160, Loss: 3.148256028704631\n",
            "Epoch 71/160, Loss: 3.149618332343333\n",
            "Epoch 72/160, Loss: 3.135964744841046\n",
            "Epoch 73/160, Loss: 3.119209233452292\n",
            "Epoch 74/160, Loss: 3.1239979248827376\n",
            "Epoch 75/160, Loss: 3.107482576614146\n",
            "Epoch 76/160, Loss: 3.101577867327444\n",
            "Epoch 77/160, Loss: 3.0868116644642236\n",
            "Epoch 78/160, Loss: 3.067016530219856\n",
            "Epoch 79/160, Loss: 3.0585124212152817\n",
            "Epoch 80/160, Loss: 3.0341626670964232\n",
            "Epoch 81/160, Loss: 3.029484895793983\n",
            "Epoch 82/160, Loss: 3.0173202432939767\n",
            "Epoch 83/160, Loss: 3.00573709675723\n",
            "Epoch 84/160, Loss: 2.9913765957288425\n",
            "Epoch 85/160, Loss: 2.983966220675222\n",
            "Epoch 86/160, Loss: 2.978686089405928\n",
            "Epoch 87/160, Loss: 2.966698147756669\n",
            "Epoch 88/160, Loss: 2.9550487769534217\n",
            "Epoch 89/160, Loss: 2.9594475966890146\n",
            "Epoch 90/160, Loss: 2.956790542968399\n",
            "Epoch 91/160, Loss: 2.9493758788194193\n",
            "Epoch 92/160, Loss: 2.945351941506271\n",
            "Epoch 93/160, Loss: 2.9352089651405353\n",
            "Epoch 94/160, Loss: 2.930523316268726\n",
            "Epoch 95/160, Loss: 2.9297290299554617\n",
            "Epoch 96/160, Loss: 2.9148750317371106\n",
            "Epoch 97/160, Loss: 2.9211742115752473\n",
            "Epoch 98/160, Loss: 2.9180847677733284\n",
            "Epoch 99/160, Loss: 2.9111458340569225\n",
            "Epoch 100/160, Loss: 2.906800368862689\n",
            "Epoch 101/160, Loss: 2.8957400632941206\n",
            "Epoch 102/160, Loss: 2.879167340905465\n",
            "Epoch 103/160, Loss: 2.8709614472011165\n",
            "Epoch 104/160, Loss: 2.858595074290205\n",
            "Epoch 105/160, Loss: 2.846379784976735\n",
            "Epoch 106/160, Loss: 2.831883326820705\n",
            "Epoch 107/160, Loss: 2.8334481386882264\n",
            "Epoch 108/160, Loss: 2.8220827308152336\n",
            "Epoch 109/160, Loss: 2.8147487070249473\n",
            "Epoch 110/160, Loss: 2.8042727425275253\n",
            "Epoch 111/160, Loss: 2.809724511392891\n",
            "Epoch 112/160, Loss: 2.7947696337614523\n",
            "Epoch 113/160, Loss: 2.797746690033037\n",
            "Epoch 114/160, Loss: 2.7812020970732356\n",
            "Epoch 115/160, Loss: 2.7820444210715918\n",
            "Epoch 116/160, Loss: 2.784412960872016\n",
            "Epoch 117/160, Loss: 2.770943294095871\n",
            "Epoch 118/160, Loss: 2.773530604589321\n",
            "Epoch 119/160, Loss: 2.775452991275836\n",
            "Epoch 120/160, Loss: 2.759274511995828\n",
            "Epoch 121/160, Loss: 2.7634501304772807\n",
            "Epoch 122/160, Loss: 2.758145200017163\n",
            "Epoch 123/160, Loss: 2.7548776319264756\n",
            "Epoch 124/160, Loss: 2.7575653062757017\n",
            "Epoch 125/160, Loss: 2.762687370295415\n",
            "Epoch 126/160, Loss: 2.7591579722626434\n",
            "Epoch 127/160, Loss: 2.7463802937656414\n",
            "Epoch 128/160, Loss: 2.7465967656401418\n",
            "Epoch 129/160, Loss: 2.732667322963705\n",
            "Epoch 130/160, Loss: 2.742675914788795\n",
            "Epoch 131/160, Loss: 2.7334691380600793\n",
            "Epoch 132/160, Loss: 2.7219042290202187\n",
            "Epoch 133/160, Loss: 2.7307832064226156\n",
            "Epoch 134/160, Loss: 2.718312409222888\n",
            "Epoch 135/160, Loss: 2.730943270656459\n",
            "Epoch 136/160, Loss: 2.7295613709618065\n",
            "Epoch 137/160, Loss: 2.7228006298279825\n",
            "Epoch 138/160, Loss: 2.7246795907959607\n",
            "Epoch 139/160, Loss: 2.71220072638958\n",
            "Epoch 140/160, Loss: 2.714512913428304\n",
            "Epoch 141/160, Loss: 2.7175207662460443\n",
            "Epoch 142/160, Loss: 2.714993289669456\n",
            "Epoch 143/160, Loss: 2.6911039666446577\n",
            "Epoch 144/160, Loss: 2.7105673249725184\n",
            "Epoch 145/160, Loss: 2.70065937444682\n",
            "Epoch 146/160, Loss: 2.7022559594010453\n",
            "Epoch 147/160, Loss: 2.7032763305527476\n",
            "Epoch 148/160, Loss: 2.6919665074409425\n",
            "Epoch 149/160, Loss: 2.6946018772661837\n",
            "Epoch 150/160, Loss: 2.698283251594095\n",
            "Epoch 151/160, Loss: 2.6954730447295985\n",
            "Epoch 152/160, Loss: 2.686975033691777\n",
            "Epoch 153/160, Loss: 2.7022000554272587\n",
            "Epoch 154/160, Loss: 2.699291651206248\n",
            "Epoch 155/160, Loss: 2.68172868438389\n",
            "Epoch 156/160, Loss: 2.686474366566104\n",
            "Epoch 157/160, Loss: 2.6846237182617188\n",
            "Epoch 158/160, Loss: 2.679821533315322\n",
            "Epoch 159/160, Loss: 2.6690725508858177\n",
            "Epoch 160/160, Loss: 2.6819635120499163\n",
            "Epoch 1/160, Loss: 6.7113603216302975\n",
            "Epoch 2/160, Loss: 5.84832616718224\n",
            "Epoch 3/160, Loss: 5.605215900694318\n",
            "Epoch 4/160, Loss: 5.438909856254792\n",
            "Epoch 5/160, Loss: 5.276268809042928\n",
            "Epoch 6/160, Loss: 5.165374814396929\n",
            "Epoch 7/160, Loss: 5.075884722687704\n",
            "Epoch 8/160, Loss: 5.0258315383930645\n",
            "Epoch 9/160, Loss: 4.983624640023312\n",
            "Epoch 10/160, Loss: 4.953481919320343\n",
            "Epoch 11/160, Loss: 4.922655162908842\n",
            "Epoch 12/160, Loss: 4.894393475769121\n",
            "Epoch 13/160, Loss: 4.872954046634762\n",
            "Epoch 14/160, Loss: 4.849658052634705\n",
            "Epoch 15/160, Loss: 4.834417707779828\n",
            "Epoch 16/160, Loss: 4.820431612946493\n",
            "Epoch 17/160, Loss: 4.787927761712037\n",
            "Epoch 18/160, Loss: 4.754734427117936\n",
            "Epoch 19/160, Loss: 4.7320008412041625\n",
            "Epoch 20/160, Loss: 4.707213257889613\n",
            "Epoch 21/160, Loss: 4.697023446602589\n",
            "Epoch 22/160, Loss: 4.682717914776424\n",
            "Epoch 23/160, Loss: 4.666789600001577\n",
            "Epoch 24/160, Loss: 4.656138279858758\n",
            "Epoch 25/160, Loss: 4.647090641129047\n",
            "Epoch 26/160, Loss: 4.6353595201926465\n",
            "Epoch 27/160, Loss: 4.630608720242825\n",
            "Epoch 28/160, Loss: 4.630911823428805\n",
            "Epoch 29/160, Loss: 4.616872064292888\n",
            "Epoch 30/160, Loss: 4.614979004920901\n",
            "Epoch 31/160, Loss: 4.608495146417252\n",
            "Epoch 32/160, Loss: 4.60513196515915\n",
            "Epoch 33/160, Loss: 4.5971211484631\n",
            "Epoch 34/160, Loss: 4.5950272138161425\n",
            "Epoch 35/160, Loss: 4.589407935471791\n",
            "Epoch 36/160, Loss: 4.588302085466702\n",
            "Epoch 37/160, Loss: 4.5870261521595515\n",
            "Epoch 38/160, Loss: 4.579066460699681\n",
            "Epoch 39/160, Loss: 4.578302083417888\n",
            "Epoch 40/160, Loss: 4.576685994482406\n",
            "Epoch 41/160, Loss: 4.571627035775148\n",
            "Epoch 42/160, Loss: 4.57727054622777\n",
            "Epoch 43/160, Loss: 4.565230193955209\n",
            "Epoch 44/160, Loss: 4.566573880822458\n",
            "Epoch 45/160, Loss: 4.559949583409693\n",
            "Epoch 46/160, Loss: 4.565785912906422\n",
            "Epoch 47/160, Loss: 4.564144416233463\n",
            "Epoch 48/160, Loss: 4.550356109428893\n",
            "Epoch 49/160, Loss: 4.55400332160618\n",
            "Epoch 50/160, Loss: 4.545345557620154\n",
            "Epoch 51/160, Loss: 4.547846810592105\n",
            "Epoch 52/160, Loss: 4.550957238277816\n",
            "Epoch 53/160, Loss: 4.549011376812635\n",
            "Epoch 54/160, Loss: 4.54544336289701\n",
            "Epoch 55/160, Loss: 4.542093685520884\n",
            "Epoch 56/160, Loss: 4.543469381454351\n",
            "Epoch 57/160, Loss: 4.541348646363944\n",
            "Epoch 58/160, Loss: 4.546807987915585\n",
            "Epoch 59/160, Loss: 4.5390842271887735\n",
            "Epoch 60/160, Loss: 4.538713904597875\n",
            "Epoch 61/160, Loss: 4.541365911893528\n",
            "Epoch 62/160, Loss: 4.537343246552646\n",
            "Epoch 63/160, Loss: 4.529632032984662\n",
            "Epoch 64/160, Loss: 4.5356381140706485\n"
          ]
        }
      ]
    }
  ]
}